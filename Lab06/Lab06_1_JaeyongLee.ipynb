{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"lab-06_1_softmax_classification.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"xj8pbMWxbXmT"},"source":["# Lab 6-1: Softmax Classification"]},{"cell_type":"markdown","metadata":{"id":"VKOx9AItbXmW"},"source":["Author: Seungjae Lee (이승재)"]},{"cell_type":"markdown","metadata":{"id":"TKtUD0yZbXmW"},"source":["<div class=\"alert alert-warning\">\n","    We use elemental PyTorch to implement linear regression here. However, in most actual applications, abstractions such as <code>nn.Module</code> or <code>nn.Linear</code> are used.\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"t9AtaChDbXmX"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"Xsigf9uebXmY"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6d_SV5ARbXmZ","outputId":"749dd59b-22f9-4421-d213-4fe6ecb822f9"},"source":["# For reproducibility\n","torch.manual_seed(1)"],"execution_count":null,"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x105eccfb0>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"nlQzMnIBbXmc"},"source":["## Softmax"]},{"cell_type":"markdown","metadata":{"id":"qKSDY_q3bXmc"},"source":["Convert numbers to probabilities with softmax."]},{"cell_type":"markdown","metadata":{"id":"e6BsPuGXbXmd"},"source":["$$ P(class=i) = \\frac{e^i}{\\sum e^i} $$"]},{"cell_type":"code","metadata":{"id":"4QW-DYb1bXmd"},"source":["z = torch.FloatTensor([1, 2, 3])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Yax3BQg1bXme"},"source":["PyTorch has a `softmax` function."]},{"cell_type":"code","metadata":{"id":"9eYWaOx_bXme","outputId":"6290beeb-27c8-446d-a42a-80d51fe50c21"},"source":["hypothesis = F.softmax(z, dim=0)\n","print(hypothesis)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0.0900, 0.2447, 0.6652])\n"]}]},{"cell_type":"markdown","metadata":{"id":"3EAXrBiGbXmf"},"source":["Since they are probabilities, they should add up to 1. Let's do a sanity check."]},{"cell_type":"code","metadata":{"id":"gQXUvUOVbXmg","outputId":"32789835-dba3-4111-9448-a7a54f363b8f"},"source":["hypothesis.sum()"],"execution_count":null,"outputs":[{"data":{"text/plain":["tensor(1.)"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"A3aCUZNXbXmg"},"source":["## Cross Entropy Loss (Low-level)"]},{"cell_type":"markdown","metadata":{"id":"lwE-JKOIbXmh"},"source":["For multi-class classification, we use the cross entropy loss."]},{"cell_type":"markdown","metadata":{"id":"Ul0AxNxEbXmi"},"source":["$$ L = \\frac{1}{N} \\sum - y \\log(\\hat{y}) $$"]},{"cell_type":"markdown","metadata":{"id":"Ub9VEJIibXmj"},"source":["where $\\hat{y}$ is the predicted probability and $y$ is the correct probability (0 or 1)."]},{"cell_type":"code","metadata":{"scrolled":true,"id":"wNu9SC5rbXmk","outputId":"f1bf6283-2a0d-4b09-eedb-75ddf2292408"},"source":["z = torch.rand(3, 5, requires_grad=True)    #classes = 5\n","hypothesis = F.softmax(z, dim=1)            #samples = 3\n","print(hypothesis)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n","        [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n","        [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward>)\n"]}]},{"cell_type":"code","metadata":{"id":"kULmlzv2bXmk","outputId":"bc3fb555-7c3c-4ce7-9f2a-a3ec02c0b26a"},"source":["y = torch.randint(5, (3,)).long()\n","print(y)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([0, 2, 1])\n"]}]},{"cell_type":"code","metadata":{"id":"7XfIaajMbXml","outputId":"0d0e28e8-e4ff-4b25-c20a-4bc7974e9cd5"},"source":["y_one_hot = torch.zeros_like(hypothesis)\n","y_one_hot.scatter_(1, y.unsqueeze(1), 1)"],"execution_count":null,"outputs":[{"data":{"text/plain":["tensor([[1., 0., 0., 0., 0.],\n","        [0., 0., 1., 0., 0.],\n","        [0., 1., 0., 0., 0.]])"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"7K8DN1yQbXmm","outputId":"6ff43382-377f-4065-e231-1ba40bb4402d"},"source":["cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n","print(cost)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(1.4689, grad_fn=<MeanBackward1>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"WcEaHj0-bXmn"},"source":["## Cross-entropy Loss with `torch.nn.functional`"]},{"cell_type":"markdown","metadata":{"id":"GgKVy3zgbXmn"},"source":["PyTorch has `F.log_softmax()` function."]},{"cell_type":"code","metadata":{"id":"IKK4lrajbXmo","outputId":"564b35ef-8979-4ff7-e7c4-fd237448bfef"},"source":["# Low level\n","torch.log(F.softmax(z, dim=1))"],"execution_count":null,"outputs":[{"data":{"text/plain":["tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n","        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n","        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]], grad_fn=<LogBackward>)"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"UbC3qZMLbXmo","outputId":"037699ba-4a83-49b5-dfb6-1825a43058ed"},"source":["# High level\n","F.log_softmax(z, dim=1)"],"execution_count":null,"outputs":[{"data":{"text/plain":["tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n","        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n","        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]],\n","       grad_fn=<LogSoftmaxBackward>)"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"5e6G8A9TbXmp"},"source":["PyTorch also has `F.nll_loss()` function that computes the negative loss likelihood."]},{"cell_type":"code","metadata":{"id":"Azzd2I2dbXmp","outputId":"5ef47972-4705-4ef3-c45b-915ed9829520"},"source":["# Low level\n","(y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()"],"execution_count":null,"outputs":[{"data":{"text/plain":["tensor(1.4689, grad_fn=<MeanBackward1>)"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"code","metadata":{"id":"8_lBBiQzbXmr","outputId":"b1a147a2-3bd1-48e6-bda7-b2389e8e13c6"},"source":["# High level\n","F.nll_loss(F.log_softmax(z, dim=1), y)"],"execution_count":null,"outputs":[{"data":{"text/plain":["tensor(1.4689, grad_fn=<NllLossBackward>)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"kp11KuxabXmr"},"source":["PyTorch also has `F.cross_entropy` that combines `F.log_softmax()` and `F.nll_loss()`."]},{"cell_type":"code","metadata":{"id":"6AK2p7U6bXms","outputId":"9623e36c-c257-4eae-9507-ebd48a6cc211"},"source":["F.cross_entropy(z, y)"],"execution_count":null,"outputs":[{"data":{"text/plain":["tensor(1.4689, grad_fn=<NllLossBackward>)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"tnNU38pPbXms"},"source":["## Training with Low-level Cross Entropy Loss"]},{"cell_type":"code","metadata":{"id":"GV20ZOkWbXmt"},"source":["x_train = [[1, 2, 1, 1],\n","           [2, 1, 3, 2],\n","           [3, 1, 3, 4],\n","           [4, 1, 5, 5],\n","           [1, 7, 5, 5],\n","           [1, 2, 5, 6],\n","           [1, 6, 6, 6],\n","           [1, 7, 7, 7]]\n","y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n","x_train = torch.FloatTensor(x_train)\n","y_train = torch.LongTensor(y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ooSWkk6bbXmt","outputId":"c350accb-a6f8-4b68-ffdb-001f0495b5cf"},"source":["# 모델 초기화\n","W = torch.zeros((4, 3), requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=0.1)\n","\n","nb_epochs = 1000\n","for epoch in range(nb_epochs + 1):\n","\n","    # Cost 계산 (1)\n","    hypothesis = F.softmax(x_train.matmul(W) + b, dim=1) # or .mm or @\n","    y_one_hot = torch.zeros_like(hypothesis)\n","    y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n","    cost = (y_one_hot * -torch.log(F.softmax(hypothesis, dim=1))).sum(dim=1).mean()\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    # 100번마다 로그 출력\n","    if epoch % 100 == 0:\n","        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","            epoch, nb_epochs, cost.item()\n","        ))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch    0/1000 Cost: 1.098612\n","Epoch  100/1000 Cost: 0.901535\n","Epoch  200/1000 Cost: 0.839114\n","Epoch  300/1000 Cost: 0.807826\n","Epoch  400/1000 Cost: 0.788472\n","Epoch  500/1000 Cost: 0.774822\n","Epoch  600/1000 Cost: 0.764449\n","Epoch  700/1000 Cost: 0.756191\n","Epoch  800/1000 Cost: 0.749398\n","Epoch  900/1000 Cost: 0.743671\n","Epoch 1000/1000 Cost: 0.738749\n"]}]},{"cell_type":"markdown","metadata":{"id":"LX1XwoAFbXmu"},"source":["## Training with `F.cross_entropy`"]},{"cell_type":"code","metadata":{"id":"5r2RJ21MbXmv","outputId":"f1a6106e-a304-49c8-dfd0-4534d84315db"},"source":["# 모델 초기화\n","W = torch.zeros((4, 3), requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=0.1)\n","\n","nb_epochs = 1000\n","for epoch in range(nb_epochs + 1):\n","\n","    # Cost 계산 (2)\n","    z = x_train.matmul(W) + b # or .mm or @\n","    cost = F.cross_entropy(z, y_train)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    # 100번마다 로그 출력\n","    if epoch % 100 == 0:\n","        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","            epoch, nb_epochs, cost.item()\n","        ))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch    0/1000 Cost: 1.098612\n","Epoch  100/1000 Cost: 0.761050\n","Epoch  200/1000 Cost: 0.689991\n","Epoch  300/1000 Cost: 0.643229\n","Epoch  400/1000 Cost: 0.604117\n","Epoch  500/1000 Cost: 0.568255\n","Epoch  600/1000 Cost: 0.533922\n","Epoch  700/1000 Cost: 0.500291\n","Epoch  800/1000 Cost: 0.466908\n","Epoch  900/1000 Cost: 0.433507\n","Epoch 1000/1000 Cost: 0.399962\n"]}]},{"cell_type":"markdown","metadata":{"id":"utcgjEjvbXmw"},"source":["## High-level Implementation with `nn.Module`"]},{"cell_type":"code","metadata":{"id":"0LO9g2hcbXmw"},"source":["class SoftmaxClassifierModel(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.linear = nn.Linear(4, 3) # Output이 3!\n","\n","    def forward(self, x):\n","        return self.linear(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cy5iUUONbXmx"},"source":["model = SoftmaxClassifierModel()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ttj3me4hbXmx"},"source":["Let's try another new dataset."]},{"cell_type":"code","metadata":{"id":"TS9FOAimbXmx","outputId":"e9974de1-26d8-4c96-8897-ec95b3c9d8d1"},"source":["# optimizer 설정\n","optimizer = optim.SGD(model.parameters(), lr=0.1)\n","\n","nb_epochs = 1000\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    prediction = model(x_train)\n","\n","    # cost 계산\n","    cost = F.cross_entropy(prediction, y_train)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","    \n","    # 20번마다 로그 출력\n","    if epoch % 100 == 0:\n","        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","            epoch, nb_epochs, cost.item()\n","        ))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch    0/1000 Cost: 1.849513\n","Epoch  100/1000 Cost: 0.689894\n","Epoch  200/1000 Cost: 0.609259\n","Epoch  300/1000 Cost: 0.551218\n","Epoch  400/1000 Cost: 0.500141\n","Epoch  500/1000 Cost: 0.451947\n","Epoch  600/1000 Cost: 0.405051\n","Epoch  700/1000 Cost: 0.358733\n","Epoch  800/1000 Cost: 0.312912\n","Epoch  900/1000 Cost: 0.269521\n","Epoch 1000/1000 Cost: 0.241922\n"]}]}]}