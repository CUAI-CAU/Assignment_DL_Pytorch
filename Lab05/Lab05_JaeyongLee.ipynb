{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.5"},"colab":{"name":"lab-05_logistic_classification.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"3a-4vilVUpDL"},"source":["# Lab 5: Logistic Classification"]},{"cell_type":"markdown","metadata":{"id":"NugC44p9UpDP"},"source":["Author: Seungjae Lee (이승재)"]},{"cell_type":"markdown","metadata":{"id":"ERoNG5qvUpDQ"},"source":["<div class=\"alert alert-warning\">\n","    We use elemental PyTorch to implement linear regression here. However, in most actual applications, abstractions such as <code>nn.Module</code> or <code>nn.Linear</code> are used. You can see those implementations near the end of this notebook.\n","</div>"]},{"cell_type":"markdown","metadata":{"id":"3WEXzcVgUpDR"},"source":["## Reminder: Logistic Regression"]},{"cell_type":"markdown","metadata":{"id":"IotzzfyKUpDS"},"source":["### Hypothesis"]},{"cell_type":"markdown","metadata":{"id":"P6WyXSNTUpDS"},"source":["$$ H(X) = \\frac{1}{1+e^{-W^T X}} $$"]},{"cell_type":"markdown","metadata":{"id":"253eDyoyUpDS"},"source":["### Cost"]},{"cell_type":"markdown","metadata":{"id":"nH_3HRJuUpDT"},"source":["$$ cost(W) = -\\frac{1}{m} \\sum y \\log\\left(H(x)\\right) + (1-y) \\left( \\log(1-H(x) \\right) $$"]},{"cell_type":"markdown","metadata":{"id":"yEAIcVm_UpDT"},"source":[" - If $y \\simeq H(x)$, cost is near 0.\n"," - If $y \\neq H(x)$, cost is high."]},{"cell_type":"markdown","metadata":{"id":"nnBE6ileUpDU"},"source":["### Weight Update via Gradient Descent"]},{"cell_type":"markdown","metadata":{"id":"c0X_OzKdUpDU"},"source":["$$ W := W - \\alpha \\frac{\\partial}{\\partial W} cost(W) $$"]},{"cell_type":"markdown","metadata":{"id":"G2NNYNOBUpDV"},"source":[" - $\\alpha$: Learning rate"]},{"cell_type":"markdown","metadata":{"id":"db-lyx8BUpDV"},"source":["## Imports"]},{"cell_type":"code","metadata":{"id":"zZpP2apWUpDV"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pMPlFadZUpDX","outputId":"1f21c99f-1f0e-4d65-9da2-e4eb33d918fe"},"source":["# For reproducibility\n","torch.manual_seed(1)"],"execution_count":null,"outputs":[{"data":{"text/plain":["<torch._C.Generator at 0x106951ed0>"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"D0LJIp_NUpDZ"},"source":["## Training Data"]},{"cell_type":"code","metadata":{"id":"z0v6CWu9UpDZ"},"source":["x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]  #(6,2)\n","y_data = [[0], [0], [0], [1], [1], [1]]  #(6,1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cpx-oh21UpDa"},"source":["Consider the following classification problem: given the number of hours each student spent watching the lecture and working in the code lab, predict whether the student passed or failed a course. For example, the first (index 0) student watched the lecture for 1 hour and spent 2 hours in the lab session ([1, 2]), and ended up failing the course ([0])."]},{"cell_type":"code","metadata":{"id":"BIqUKK4YUpDa"},"source":["x_train = torch.FloatTensor(x_data)  #Converting data to torch.Tensor format\n","y_train = torch.FloatTensor(y_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b8thSrnJUpDa"},"source":["As always, we need these data to be in `torch.Tensor` format, so we convert them."]},{"cell_type":"code","metadata":{"id":"hWmpCr9xUpDb","outputId":"ad1528db-bf49-49f6-8491-297c841fff72"},"source":["print(x_train.shape)\n","print(y_train.shape)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([6, 2])\n","torch.Size([6, 1])\n"]}]},{"cell_type":"markdown","metadata":{"id":"PkxEQf4XUpDc"},"source":["## Computing the Hypothesis"]},{"cell_type":"markdown","metadata":{"id":"lyJAddWmUpDc"},"source":["$$ H(X) = \\frac{1}{1+e^{-W^T X}} $$"]},{"cell_type":"markdown","metadata":{"id":"mp6s_mHgUpDc"},"source":["PyTorch has a `torch.exp()` function that resembles the exponential function."]},{"cell_type":"code","metadata":{"id":"ylkif4RgUpDd","outputId":"605528c8-9a91-4a48-cf41-3f739a13a6a7"},"source":["print('e^1 equals: ', torch.exp(torch.FloatTensor([1])))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["e^1 equals:  tensor([2.7183])\n"]}]},{"cell_type":"markdown","metadata":{"id":"0EBhRwj5UpDe"},"source":["We can use it to compute the hypothesis function conveniently."]},{"cell_type":"code","metadata":{"id":"3tkyfpeQUpDe"},"source":["W = torch.zeros((2, 1), requires_grad=True)  #Implementing Gradient\n","b = torch.zeros(1, requires_grad=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bgob_sH0UpDe"},"source":["hypothesis = 1 / (1 + torch.exp(-(x_train.matmul(W) + b)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B0GchDo_UpDf","outputId":"76d5afd2-df96-4d26-ccdd-88fdecafaf06"},"source":["print(hypothesis)\n","print(hypothesis.shape)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.5000],\n","        [0.5000],\n","        [0.5000],\n","        [0.5000],\n","        [0.5000],\n","        [0.5000]], grad_fn=<MulBackward>)\n","torch.Size([6, 1])\n"]}]},{"cell_type":"markdown","metadata":{"id":"_LAxRwDjUpDf"},"source":["Or, we could use `torch.sigmoid()` function! This resembles the sigmoid function:"]},{"cell_type":"code","metadata":{"id":"A0yeqkKcUpDg","outputId":"6b139d73-f4d3-43a4-8a59-53ea24b1b06f"},"source":["print('1/(1+e^{-1}) equals: ', torch.sigmoid(torch.FloatTensor([1])))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["1/(1+e^{-1}) equals:  tensor([0.7311])\n"]}]},{"cell_type":"markdown","metadata":{"id":"nz7qpW2-UpDh"},"source":["Now, the code for hypothesis function is cleaner."]},{"cell_type":"code","metadata":{"id":"6mgpk4qFUpDh"},"source":["hypothesis = torch.sigmoid(x_train.matmul(W) + b)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xC_oqRAlUpDh","outputId":"ea121c0d-129b-4eda-e66b-d2a0ebfe793f"},"source":["print(hypothesis)\n","print(hypothesis.shape)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.5000],\n","        [0.5000],\n","        [0.5000],\n","        [0.5000],\n","        [0.5000],\n","        [0.5000]], grad_fn=<SigmoidBackward>)\n","torch.Size([6, 1])\n"]}]},{"cell_type":"markdown","metadata":{"id":"aB6DV_YKUpDi"},"source":["## Computing the Cost Function (Low-level)"]},{"cell_type":"markdown","metadata":{"id":"RHNNSM4MUpDj"},"source":["$$ cost(W) = -\\frac{1}{m} \\sum y \\log\\left(H(x)\\right) + (1-y) \\left( \\log(1-H(x) \\right) $$"]},{"cell_type":"markdown","metadata":{"id":"lc3sL31sUpDj"},"source":["We want to measure the difference between `hypothesis` and `y_train`."]},{"cell_type":"code","metadata":{"id":"MOkIy5m5UpDk","outputId":"4b71428d-4167-40f9-e1cc-dd01345b2a7a"},"source":["print(hypothesis)\n","print(y_train)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.5000],\n","        [0.5000],\n","        [0.5000],\n","        [0.5000],\n","        [0.5000],\n","        [0.5000]], grad_fn=<SigmoidBackward>)\n","tensor([[0.],\n","        [0.],\n","        [0.],\n","        [1.],\n","        [1.],\n","        [1.]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"53EVfMV6UpDk"},"source":["For one element, the loss can be computed as follows:"]},{"cell_type":"code","metadata":{"id":"0VkDIyZeUpDl","outputId":"3657b6df-dece-4725-9832-38c1eb12f51a"},"source":["-(y_train[0] * torch.log(hypothesis[0]) + \n","  (1 - y_train[0]) * torch.log(1 - hypothesis[0]))"],"execution_count":null,"outputs":[{"data":{"text/plain":["tensor([0.6931], grad_fn=<NegBackward>)"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"j1HpqTTYUpDl"},"source":["To compute the losses for the entire batch, we can simply input the entire vector."]},{"cell_type":"code","metadata":{"id":"2CX3dMf4UpDm","outputId":"eaddddbb-f63f-4588-dab6-31097b4ba6f7"},"source":["losses = -(y_train * torch.log(hypothesis) + \n","           (1 - y_train) * torch.log(1 - hypothesis))\n","print(losses)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.6931],\n","        [0.6931],\n","        [0.6931],\n","        [0.6931],\n","        [0.6931],\n","        [0.6931]], grad_fn=<NegBackward>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"kD66EXECUpDm"},"source":["Then, we just `.mean()` to take the mean of these individual losses."]},{"cell_type":"code","metadata":{"id":"l99sGYAaUpDn","outputId":"63258688-daeb-48a4-a345-a15db737c8cd"},"source":["cost = losses.mean()\n","print(cost)"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(0.6931, grad_fn=<MeanBackward1>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"1oaHPzvQUpDo"},"source":["## Computing the Cost Function with `F.binary_cross_entropy`"]},{"cell_type":"markdown","metadata":{"id":"-dD5VmuCUpDo"},"source":["In reality, binary classification is used so often that PyTorch has a simple function called `F.binary_cross_entropy` implemented to lighten the burden."]},{"cell_type":"code","metadata":{"id":"Mscn12PQUpDo","outputId":"aae2bb3f-884c-4b8a-f80f-07f7c1786c11"},"source":["F.binary_cross_entropy(hypothesis, y_train)"],"execution_count":null,"outputs":[{"data":{"text/plain":["tensor(0.6931, grad_fn=<BinaryCrossEntropyBackward>)"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}]},{"cell_type":"markdown","metadata":{"id":"RaMkYfRfUpDp"},"source":["## Training with Low-level Binary Cross Entropy Loss"]},{"cell_type":"code","metadata":{"id":"iMRxc8dgUpDp"},"source":["x_data = [[1, 2], [2, 3], [3, 1], [4, 3], [5, 3], [6, 2]]\n","y_data = [[0], [0], [0], [1], [1], [1]]\n","x_train = torch.FloatTensor(x_data)\n","y_train = torch.FloatTensor(y_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YDNUQ4TWUpDq","outputId":"b6402126-ee2f-416c-e3cf-7d58dcfc8692"},"source":["# 모델 초기화\n","W = torch.zeros((2, 1), requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=1)\n","\n","nb_epochs = 1000\n","for epoch in range(nb_epochs + 1):\n","\n","    # Cost 계산\n","    hypothesis = torch.sigmoid(x_train.matmul(W) + b) # or .mm or @\n","    cost = -(y_train * torch.log(hypothesis) + \n","             (1 - y_train) * torch.log(1 - hypothesis)).mean()\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    # 100번마다 로그 출력\n","    if epoch % 100 == 0:\n","        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","            epoch, nb_epochs, cost.item()\n","        ))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch    0/1000 Cost: 0.693147\n","Epoch  100/1000 Cost: 0.134722\n","Epoch  200/1000 Cost: 0.080643\n","Epoch  300/1000 Cost: 0.057900\n","Epoch  400/1000 Cost: 0.045300\n","Epoch  500/1000 Cost: 0.037261\n","Epoch  600/1000 Cost: 0.031673\n","Epoch  700/1000 Cost: 0.027556\n","Epoch  800/1000 Cost: 0.024394\n","Epoch  900/1000 Cost: 0.021888\n","Epoch 1000/1000 Cost: 0.019852\n"]}]},{"cell_type":"markdown","metadata":{"id":"fv7tDIplUpDq"},"source":["## Training with `F.binary_cross_entropy`"]},{"cell_type":"code","metadata":{"id":"mjcQn1lQUpDr","outputId":"2bb1cb88-4bd3-4458-e725-fbbc69db3c7e"},"source":["# 모델 초기화\n","W = torch.zeros((2, 1), requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=1)\n","\n","nb_epochs = 1000\n","for epoch in range(nb_epochs + 1):\n","\n","    # Cost 계산\n","    hypothesis = torch.sigmoid(x_train.matmul(W) + b) # or .mm or @\n","    cost = F.binary_cross_entropy(hypothesis, y_train)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    # 100번마다 로그 출력\n","    if epoch % 100 == 0:\n","        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","            epoch, nb_epochs, cost.item()\n","        ))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch    0/1000 Cost: 0.693147\n","Epoch  100/1000 Cost: 0.134722\n","Epoch  200/1000 Cost: 0.080643\n","Epoch  300/1000 Cost: 0.057900\n","Epoch  400/1000 Cost: 0.045300\n","Epoch  500/1000 Cost: 0.037261\n","Epoch  600/1000 Cost: 0.031672\n","Epoch  700/1000 Cost: 0.027556\n","Epoch  800/1000 Cost: 0.024394\n","Epoch  900/1000 Cost: 0.021888\n","Epoch 1000/1000 Cost: 0.019852\n"]}]},{"cell_type":"markdown","metadata":{"id":"Pyee3pw8UpDr"},"source":["## Loading Real Data"]},{"cell_type":"code","metadata":{"id":"ECaUbl07UpDs"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xxRyLhv8UpDs"},"source":["xy = np.loadtxt('data-03-diabetes.csv', delimiter=',', dtype=np.float32)\n","x_data = xy[:, 0:-1]\n","y_data = xy[:, [-1]]\n","x_train = torch.FloatTensor(x_data)\n","y_train = torch.FloatTensor(y_data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3YKtnIQgUpDt","outputId":"69ad179b-51df-4651-b0c7-8f72730e83c6"},"source":["print(x_train[0:5])\n","print(y_train[0:5])"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-0.2941,  0.4874,  0.1803, -0.2929,  0.0000,  0.0015, -0.5312, -0.0333],\n","        [-0.8824, -0.1457,  0.0820, -0.4141,  0.0000, -0.2072, -0.7669, -0.6667],\n","        [-0.0588,  0.8392,  0.0492,  0.0000,  0.0000, -0.3055, -0.4927, -0.6333],\n","        [-0.8824, -0.1055,  0.0820, -0.5354, -0.7778, -0.1624, -0.9240,  0.0000],\n","        [ 0.0000,  0.3769, -0.3443, -0.2929, -0.6028,  0.2846,  0.8873, -0.6000]])\n","tensor([[0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.]])\n"]}]},{"cell_type":"markdown","metadata":{"id":"QhhdaVnyUpDt"},"source":["## Training with Real Data using low-level Binary Cross Entropy Loss"]},{"cell_type":"code","metadata":{"id":"mHXoOjROUpDu","outputId":"ffcb2a8e-1d5c-4ea4-ea6d-f7f7cd4dd7d3"},"source":["# 모델 초기화\n","W = torch.zeros((8, 1), requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=1)\n","\n","nb_epochs = 100\n","for epoch in range(nb_epochs + 1):\n","\n","    # Cost 계산\n","    hypothesis = torch.sigmoid(x_train.matmul(W) + b) # or .mm or @\n","    cost = -(y_train * torch.log(hypothesis) + (1 - y_train) * torch.log(1 - hypothesis)).mean()\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    # 10번마다 로그 출력\n","    if epoch % 10 == 0:\n","        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","            epoch, nb_epochs, cost.item()\n","        ))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch    0/100 Cost: 0.693148\n","Epoch   10/100 Cost: 0.572727\n","Epoch   20/100 Cost: 0.539493\n","Epoch   30/100 Cost: 0.519708\n","Epoch   40/100 Cost: 0.507066\n","Epoch   50/100 Cost: 0.498539\n","Epoch   60/100 Cost: 0.492549\n","Epoch   70/100 Cost: 0.488209\n","Epoch   80/100 Cost: 0.484985\n","Epoch   90/100 Cost: 0.482543\n","Epoch  100/100 Cost: 0.480661\n"]}]},{"cell_type":"markdown","metadata":{"id":"p4FW1XKwUpDv"},"source":["## Training with Real Data using `F.binary_cross_entropy`"]},{"cell_type":"code","metadata":{"id":"MCis--SQUpDv","outputId":"d5dee585-83d5-4443-b6f6-c3f81f990ff3"},"source":["# 모델 초기화\n","W = torch.zeros((8, 1), requires_grad=True)\n","b = torch.zeros(1, requires_grad=True)\n","# optimizer 설정\n","optimizer = optim.SGD([W, b], lr=1)\n","\n","nb_epochs = 100\n","for epoch in range(nb_epochs + 1):\n","\n","    # Cost 계산\n","    hypothesis = torch.sigmoid(x_train.matmul(W) + b) # or .mm or @\n","    cost = F.binary_cross_entropy(hypothesis, y_train)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","\n","    # 10번마다 로그 출력\n","    if epoch % 10 == 0:\n","        print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n","            epoch, nb_epochs, cost.item()\n","        ))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch    0/100 Cost: 0.693147\n","Epoch   10/100 Cost: 0.572727\n","Epoch   20/100 Cost: 0.539494\n","Epoch   30/100 Cost: 0.519708\n","Epoch   40/100 Cost: 0.507065\n","Epoch   50/100 Cost: 0.498539\n","Epoch   60/100 Cost: 0.492549\n","Epoch   70/100 Cost: 0.488208\n","Epoch   80/100 Cost: 0.484985\n","Epoch   90/100 Cost: 0.482543\n","Epoch  100/100 Cost: 0.480661\n"]}]},{"cell_type":"markdown","metadata":{"id":"6rRpK8SKUpDw"},"source":["## Checking the Accuracy our our Model"]},{"cell_type":"markdown","metadata":{"id":"bDOoZU_SUpDw"},"source":["After we finish training the model, we want to check how well our model fits the training set."]},{"cell_type":"code","metadata":{"id":"cWO-957SUpDx","outputId":"e648b2d0-a5eb-46c1-a861-620d3c4c56d7"},"source":["hypothesis = torch.sigmoid(x_train.matmul(W) + b)\n","print(hypothesis[:5])"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0.4103],\n","        [0.9242],\n","        [0.2300],\n","        [0.9411],\n","        [0.1772]], grad_fn=<SliceBackward>)\n"]}]},{"cell_type":"markdown","metadata":{"id":"BIECwgqIUpDx"},"source":["We can change **hypothesis** (real number from 0 to 1) to **binary predictions** (either 0 or 1) by comparing them to 0.5."]},{"cell_type":"code","metadata":{"id":"9shgm92eUpDy","outputId":"6ca2e2c1-2296-49d2-8dde-3eb18fc2408b"},"source":["prediction = hypothesis >= torch.FloatTensor([0.5])\n","print(prediction[:5])"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0],\n","        [1],\n","        [0],\n","        [1],\n","        [0]], dtype=torch.uint8)\n"]}]},{"cell_type":"markdown","metadata":{"id":"_aEUgJsqUpDy"},"source":["Then, we compare it with the correct labels `y_train`."]},{"cell_type":"code","metadata":{"id":"pWWNsZLAUpDz","outputId":"33a423b4-4721-41f0-f79a-831a21dc8dc4"},"source":["print(prediction[:5])\n","print(y_train[:5])"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[0],\n","        [1],\n","        [0],\n","        [1],\n","        [0]], dtype=torch.uint8)\n","tensor([[0.],\n","        [1.],\n","        [0.],\n","        [1.],\n","        [0.]])\n"]}]},{"cell_type":"code","metadata":{"id":"okZj2LcKUpD1","outputId":"3c94e67c-fe29-4a7a-d19e-a2f2dcbef83a"},"source":["correct_prediction = prediction.float() == y_train\n","print(correct_prediction[:5])"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1],\n","        [1],\n","        [1],\n","        [1],\n","        [1]], dtype=torch.uint8)\n"]}]},{"cell_type":"markdown","metadata":{"id":"g-P6dIB9UpD2"},"source":["Finally, we can calculate the accuracy by counting the number of correct predictions and dividng by total number of predictions."]},{"cell_type":"code","metadata":{"id":"XZirZKAZUpD2","outputId":"e2ca10f0-ccaf-4919-fe63-f5077c7d93fb"},"source":["accuracy = correct_prediction.sum().item() / len(correct_prediction)\n","print('The model has an accuracy of {:2.2f}% for the training set.'.format(accuracy * 100))"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["The model has an accuracy of 76.68% for the training set.\n"]}]},{"cell_type":"code","metadata":{"id":"Gz97zxW4alrl"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OXppiEufUpD3"},"source":["## Optional: High-level Implementation with `nn.Module`"]},{"cell_type":"code","metadata":{"id":"si7wZIXeUpD4"},"source":["class BinaryClassifier(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.linear = nn.Linear(8, 1)   #w\n","        self.sigmoid = nn.Sigmoid()     #b\n","\n","    def forward(self, x):\n","        return self.sigmoid(self.linear(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-arvJdKmUpD4"},"source":["model = BinaryClassifier()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OjU99CwzUpD4","outputId":"f326ddeb-99e9-4bf2-c2ac-3e7d8671bec7"},"source":["# optimizer 설정\n","optimizer = optim.SGD(model.parameters(), lr=1)\n","\n","nb_epochs = 100\n","for epoch in range(nb_epochs + 1):\n","\n","    # H(x) 계산\n","    hypothesis = model(x_train)\n","\n","    # cost 계산\n","    cost = F.binary_cross_entropy(hypothesis, y_train)\n","\n","    # cost로 H(x) 개선\n","    optimizer.zero_grad()\n","    cost.backward()\n","    optimizer.step()\n","    \n","    # 20번마다 로그 출력\n","    if epoch % 10 == 0:\n","        prediction = hypothesis >= torch.FloatTensor([0.5])\n","        correct_prediction = prediction.float() == y_train\n","        accuracy = correct_prediction.sum().item() / len(correct_prediction)\n","        print('Epoch {:4d}/{} Cost: {:.6f} Accuracy {:2.2f}%'.format(\n","            epoch, nb_epochs, cost.item(), accuracy * 100,\n","        ))\n"],"execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch    0/100 Cost: 0.704829 Accuracy 45.72%\n","Epoch   10/100 Cost: 0.572391 Accuracy 67.59%\n","Epoch   20/100 Cost: 0.539563 Accuracy 73.25%\n","Epoch   30/100 Cost: 0.520042 Accuracy 75.89%\n","Epoch   40/100 Cost: 0.507561 Accuracy 76.15%\n","Epoch   50/100 Cost: 0.499125 Accuracy 76.42%\n","Epoch   60/100 Cost: 0.493177 Accuracy 77.21%\n","Epoch   70/100 Cost: 0.488846 Accuracy 76.81%\n","Epoch   80/100 Cost: 0.485612 Accuracy 76.28%\n","Epoch   90/100 Cost: 0.483146 Accuracy 76.55%\n","Epoch  100/100 Cost: 0.481234 Accuracy 76.81%\n"]}]}]}