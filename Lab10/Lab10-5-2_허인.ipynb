{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"Lab10-5-2_허인.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"0bSWzP3j7Hi4"},"source":["# VGG for cifar10"]},{"cell_type":"code","metadata":{"id":"cpN18Jxc7Hi6"},"source":["import torch\n","import torch.nn as nn\n","\n","import torch.optim as optim\n","\n","import torchvision\n","import torchvision.transforms as transforms"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"xU1DzRlH7Hi7"},"source":["import visdom\n","\n","vis = visdom.Visdom()\n","vis.close(env=\"main\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f61KCqUQ7Hi8"},"source":["def loss_tracker(loss_plot, loss_value, num):\n","    '''num, loss_value, are Tensor'''\n","    vis.line(X=num,\n","             Y=loss_value,\n","             win = loss_plot,\n","             update='append'\n","             )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Y4fbDs77Hi8"},"source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","torch.manual_seed(777)\n","if device =='cuda':\n","    torch.cuda.manual_seed_all(777)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"62lUBOgz7Hi9"},"source":["transform = transforms.Compose(\n","    [transforms.ToTensor(),\n","     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n","\n","trainset = torchvision.datasets.CIFAR10(root='./cifar10', train=True,\n","                                        download=True, transform=transform)\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=512,\n","                                          shuffle=True, num_workers=0)\n","\n","testset = torchvision.datasets.CIFAR10(root='./cifar10', train=False,\n","                                       download=True, transform=transform)\n","\n","testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n","                                         shuffle=False, num_workers=0)\n","\n","classes = ('plane', 'car', 'bird', 'cat',\n","           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5f6PqTy07Hi-"},"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","%matplotlib inline\n","\n","def imshow(img):\n","    img = img / 2 + 0.5  \n","    npimg = img.numpy()\n","    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n","    plt.show()\n","\n","# random training images\n","dataiter = iter(trainloader)\n","images, labels = dataiter.next()\n","vis.images(images/2 + 0.5)\n","\n","# show images\n","#imshow(torchvision.utils.make_grid(images))\n","\n","print(' '.join('%5s' % classes[labels[j]] for j in range(4)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oKEJcYrc7Hi-"},"source":["## make VGG16 using vgg.py"]},{"cell_type":"code","metadata":{"id":"Dz1r8Hsw7Hi_"},"source":["import vgg\n","#import torchvision.models.vgg as vgg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g7n45Qb47Hi_"},"source":["cfg = [32,32,'M', 64,64,128,128,128,'M',256,256,256,512,512,512,'M'] #13 + 3 =vgg16"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Gqoc1-r7Hi_"},"source":["class VGG(nn.Module):\n","\n","    def __init__(self, features, num_classes=1000, init_weights=True):\n","        super(VGG, self).__init__()\n","        self.features = features\n","        #self.avgpool = nn.AdaptiveAvgPool2d((7, 7))\n","        self.classifier = nn.Sequential(\n","            nn.Linear(512 * 4 * 4, 4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(4096, 4096),\n","            nn.ReLU(True),\n","            nn.Dropout(),\n","            nn.Linear(4096, num_classes),\n","        )\n","        if init_weights:\n","            self._initialize_weights()\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        #x = self.avgpool(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifier(x)\n","        return x\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n","                if m.bias is not None:\n","                    nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                nn.init.constant_(m.weight, 1)\n","                nn.init.constant_(m.bias, 0)\n","            elif isinstance(m, nn.Linear):\n","                nn.init.normal_(m.weight, 0, 0.01)\n","                nn.init.constant_(m.bias, 0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CL9-qYBc7HjA"},"source":["vgg16= VGG(vgg.make_layers(cfg),10,True).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v9AirM_e7HjA"},"source":["a=torch.Tensor(1,3,32,32).to(device)\n","out = vgg16(a)\n","print(out)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HVoKeB2G7HjA"},"source":["criterion = nn.CrossEntropyLoss().to(device)\n","optimizer = torch.optim.SGD(vgg16.parameters(), lr = 0.005,momentum=0.9)\n","\n","lr_sche = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ge-Ngz_D7HjB"},"source":["## plot"]},{"cell_type":"code","metadata":{"id":"s83KbNk67HjB"},"source":["loss_plt = vis.line(Y=torch.Tensor(1).zero_(),opts=dict(title='loss_tracker', legend=['loss'], showlegend=True))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YEegLBQ57HjB"},"source":["## training"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"mgvc1wBS7HjC"},"source":["print(len(trainloader))\n","epochs = 50\n","\n","for epoch in range(epochs):  # loop over dataset multiple times\n","    running_loss = 0.0\n","    lr_sche.step()\n","    for i, data in enumerate(trainloader, 0):\n","\n","        # get the inputs\n","        inputs, labels = data\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","\n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","\n","        # forward + backward + optimize\n","        outputs = vgg16(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","\n","        # print statistics\n","        running_loss += loss.item()\n","        if i % 30 == 29:    # print every 30 mini-batches\n","            loss_tracker(loss_plt, torch.Tensor([running_loss/30]), torch.Tensor([i + epoch*len(trainloader) ]))\n","            print('[%d, %5d] loss: %.3f' %\n","                  (epoch + 1, i + 1, running_loss / 30))\n","            running_loss = 0.0\n","        \n","print('Finished')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"C5AAzuzL7HjC"},"source":["dataiter = iter(testloader)\n","images, labels = dataiter.next()\n","\n","# print images\n","imshow(torchvision.utils.make_grid(images))\n","print('GroundTruth: ', ' '.join('%5s' % classes[labels[j]] for j in range(4)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"P2Ml_7a87HjC"},"source":["outputs = vgg16(images.to(device))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"03ioC-fi7HjD"},"source":["_, predicted = torch.max(outputs, 1)\n","\n","print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n","                              for j in range(4)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O1e8Rcds7HjD"},"source":["correct = 0\n","total = 0\n","\n","with torch.no_grad():\n","    for data in testloader:\n","        images, labels = data\n","        images = images.to(device)\n","        labels = labels.to(device)\n","        outputs = vgg16(images)\n","        \n","        _, predicted = torch.max(outputs.data, 1)\n","        \n","        total += labels.size(0)\n","        \n","        correct += (predicted == labels).sum().item()\n","\n","print('Accuracy of network on 10000 test images: %d' % (correct / total))"],"execution_count":null,"outputs":[]}]}